\section{Benchmarking Method}

A usual way to evaluate the performance of retrieval systems is to calculate
the ratio of true positive and false positive matches and visualize it in a
receiver operating characteristic (ROC) curve. While that approach is well
suited for benchmarking binary decision algorithms, it is not appropriate for
retrieval problems, that don't feature a well-defined "correct" solution. An
alternative approach is looking the recall and precision characteristics
defined as
\begin{align*}
    recall & = \frac{\text{number of correct positive results}}{\text{total number of positives}} \\
    1 - precision & = \frac{\text{number of false positive results}}{\text{total number of results}}
\end{align*}
Even though this metric works better for algorithms that return a set of
results, it is still based on the notion of a "positive match", which requires
an a-priori classification of the benchmark data. For systems that assign a
degree of similarity to each item in the result set, this would require
applying a threshold for classification. In addition to the distorting
influence of an unsuitable thresholding choice, this method completely
disregards the match quality within the positive result set, which can be
important to the user of such a retrieval system.

Since sketch-based image retrieval systems are most likely to be used in
interactive search applications of some form, it is desirable to assess the
performance in relation to the results a human would achieve.  Therefore the
benchmark used to evaluate the retrieval pipelines corresponds to the method
described by Eitz et al.\ \autocite{eitz_sketch-based_2010}, in which the
authors create a benchmark dataset and perform a user study with 28
participants to define "ground truth" rankings. The dataset is divided into 31
groups of one sketch and 40 images each. Participants ranked the 40 images
within each group by assigning scores indicating the similarity to the
corresponding sketch in a controlled study environment. Each sketch/image
pair's final ground truth ranking is calculated as the mean of the scores
assigned by all participants.

To compare a ground truth ranking $x = (x_1, x_2, \dots, x_n)$ to a ranking
$y = (y_1, y_2, \dots, y_n)$ produced by a retrieval system, the Kendall
rank correlation coefficient $\tau_B$ is used. It measures the similarity of
the orderings by grouping all pairs $p_{i, j} = \{(x_i, y_i), (x_j, y_j)\}$,
$i, j \in 1, \dots, n$ into 5 sets:
\begin{align*}
    p_{i, j} & \in C & \quad\text{if } x_i < x_j \text{ and } y_i < y_j \\
    p_{i, j} & \in D & \quad\text{if } x_i < x_j \text{ and } y_i > y_j \\
    p_{i, j} & \in T_x & \quad\text{if } x_i = x_j \text{ and } y_i \neq y_j \\
    p_{i, j} & \in T_y & \quad\text{if } x_i \neq x_j \text{ and } y_i = y_j \\
    p_{i, j} & \in T_{xy} & \quad\text{if } x_i = x_j \text{ and } y_i = y_j
\end{align*}
From that, the correlation value $\tau_B$ in the interval $[-1, 1]$ can be
calculated as
\begin{equation*}
    \tau_B = \frac{|C| - |D|}{\sqrt{(|C| + |D| + |T_x|)(|C| + |D| + |T_y|)}}.
\end{equation*}
The higher $\tau_B$ is, the more pairs in $x$ and $y$ have a similar ordering.
Since the values are only compared within each ranking, the result is
independent of each rankings' scaling, making it ideal for comparison of
different distance metrics.
